![image](https://github.com/anurag-chiplunkar/HDFC-Machine-Learning-Training/assets/59001358/132538c5-d86b-48ef-88e1-b719452b1a30)

# K-Nearest Neighbors (KNN) Algorithm

## 1. Introduction to KNN

The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful machine learning technique used for both classification and regression tasks. It is a type of instance-based learning, where the function is approximated locally and all computations are deferred until function evaluation. The KNN algorithm is based on the principle that objects that are similar will exist in close proximity to each other.

### Key Characteristics:
- **Instance-based learning**: No explicit training phase.
- **Lazy learning algorithm**: Stores all instances and delays the processing until a new instance needs to be classified.
- **Non-parametric**: Makes no underlying assumptions about the data distribution.
- **Versatility**: Can be used for classification, regression, and search problems.

## 2. Introduction to KNN Classification

KNN classification involves classifying a given data point based on how its neighbors are classified. The algorithm follows these basic steps:
1. **Choose the number of neighbors (K)**: The number of neighbors is the core deciding factor. Generally, it is an odd number to avoid ties.
2. **Calculate distance**: Compute the distance between the new data point and all existing data points using a distance metric (e.g., Euclidean distance).
3. **Sort distances**: Sort the distances in ascending order.
4. **Determine nearest neighbors**: Select the K nearest neighbors to the new data point.
5. **Majority vote**: Assign the most common class among the K neighbors to the new data point.

## 2.1 KNN Explanation on Case Study of Sexing Chicks Classification

Let's illustrate KNN classification with a case study on sexing chicks, which involves determining the sex of newly hatched chicks based on various features such as weight, wing length, and beak length.

### Case Study Steps:

1. **Collect Data**: Gather a dataset of chicks with known sexes, including features like weight, wing length, and beak length.

2. **Choose K**: Suppose we choose K = 3 (i.e., we will look at the 3 nearest neighbors).

3. **Calculate Distance**:
   - For a new chick that needs to be classified, calculate the distance from this chick to all other chicks in the dataset.
   - Example: New chick's features (weight=50g, wing length=15cm, beak length=3cm).

4. **Sort and Select Neighbors**:
   - Calculate the Euclidean distance from the new chick to each chick in the dataset.
   - Sort the distances and select the 3 nearest neighbors.

5. **Majority Vote**:
   - Suppose the 3 nearest neighbors are two male chicks and one female chick.
   - Based on the majority vote, classify the new chick as male.

### Example:

| Chick ID | Weight (g) | Wing Length (cm) | Beak Length (cm) | Sex   |
|----------|------------|------------------|------------------|-------|
| 1        | 55         | 14               | 2.9              | Male  |
| 2        | 47         | 15.2             | 3.1              | Female|
| 3        | 52         | 14.8             | 3.0              | Male  |
| ...      | ...        | ...              | ...              | ...   |
| N        | 50         | 15.0             | 3.0              | ?     |

- New chick (ID N) has features (50g, 15cm, 3cm).
- Calculate distances to all chicks.
- Sort and find the 3 nearest neighbors.
- Suppose IDs 1, 2, and 3 are nearest, with Sexes: Male, Female, Male.
- Majority vote: 2 Males vs. 1 Female.
- New chick is classified as **Male**.

KNN is intuitive and effective, particularly in scenarios where the dataset is small and the decision boundaries are irregular. However, it can be computationally intensive for large datasets and sensitive to the choice of K and the distance metric used.
